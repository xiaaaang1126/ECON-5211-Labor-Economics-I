\documentclass[11pt]{article}

\usepackage{fullpage}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{fancyvrb}

% useful package
\usepackage[obeyspaces]{url}
\usepackage{listings}  % coding blocks
\usepackage[svgnames]{xcolor}
\graphicspath{ {./pic/} }

% codign block setting
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,    captionpos=b,                    
    keepspaces=true, numbers=left,                    
    numbersep=5pt, showspaces=false,                
    showstringspaces=false, showtabs=false, tabsize=2}
\lstset{style=mystyle}


\parindent0in
\pagestyle{plain}
\thispagestyle{plain}


%% UPDATE MACRO DEFINITIONS %%
\newcommand{\myname}{Xiang Jyun, Jhang}
\newcommand{\assignment}{Homework 1}
\newcommand{\duedate}{March 10, 2024}

%% mathematical notation
\newcommand{\ept}{\mathbb{E}}
\newcommand{\normal}{\mathcal{N}}
\newcommand{\var}{\text{Var}}
\newcommand{\cov}{\text{Cov}}


\begin{document}


\textbf{National Taiwan University}\hfill\textbf{\myname}\\[0.01in]
\textbf{ECON 5211 Labor Economics I}\hfill\textbf{\assignment}\\[0.01in]
\textbf{Prof.\ Kuan Ming, Chen}\hfill\textbf{\duedate}\\
\smallskip\hrule\bigskip



% question 1
\section{Identification}

    % Q1.1
    \subsection{Heuristic Identification}
    
        

            % Q1.1.1
            \subsubsection{\textit{“We don't have enough sample size to identify the causal effects of the problem.}}
            
                \subparagraph{
                The statement is false in most cases. Since the identification is to find the parameters underlying the data generating process, sample size has nothing to do with it. However, if the sample size is very small, then the model might not be able to sucessfully identify the casual effect even if it is identifiable.}
            
            % Q1.1.2
            \subsubsection{\textit{“We don't have a good identification strategy so I need to use a structural model.”}}
            
            $\quad\Rightarrow\ $ Not having a good identification strategy can be interpreted as "Have identification set but lack method to estimate it". In this case, turning to the structural model can be a good idea.
            
            % Q1.1.3
            \subsubsection{\textit{“Because I have a structural model, I don't need to think about identification.”}}
            
            $\quad\Rightarrow\ $ The statement is false. When using structural model, identification is as crucial as reduced-form model since it helps us to understand the exact parameters underlying the data generating process

            % Q1.1.4
            \subsubsection \textit{“Because I can use the maximum likelihood estimator, I can identify that.”}
            
            $\quad\Rightarrow\ $ The statement is false and it fails to distinguish the order between identification and estimation. Only after have a correct identification can we start to discuss estimation

        

    % Q1.2
    \subsection{Identification of OLS}

    Given the OLS regression model

    \[
        y_i  =\beta x_i + \epsilon_i
    \]

    suppose the identification set is not a singleton, that is

    \[
        \beta x_i = \beta^* x_i, \quad\text{for all } x_i
    \]

    hence $\beta = \beta^*$, so the identification set is a singleton and thus $\beta$ is identified


    % Q1.3
    \subsection{Identification of a Factor Model}

    
        
        % Q1.3.1
        \subsubsection To show that \(\rho\) is identified, we consider the model equation for $y_{i,t}$:
        \[ \begin{aligned}
            y_{i,t} &= \nu_{i,t} + \epsilon_{i,t}  \\
                    &= \rho \nu_{i,t-1} + \xi_{i,t} + \epsilon_{i,t}  \\
                    &= \rho(y_{i,t-1} - \epsilon_{i,t-1}) + \xi_{i,t} + \epsilon_{i,t}  \\
                    &= \rho y_{i,t-1} + \underbrace{\xi_{i,t} + \epsilon_{i,t} - \rho \epsilon_{i,t-1}}_{\text{idiosyncratics}}
        \end{aligned} \]
        Since we can observe $y_{i,t}$ and $y_{i,t-1}$, the model equation degenrate into the OLS example in question 1.2, so $\rho$ can be identified

        % Q1.3.2
        \subsubsection We first show that $\sigma_\xi^2$ is identified by considering the equation $\nu_{i,t} = \rho \nu_{i,t-1} + \xi_{i,t}$, since the process is stationary, we have
        
        \begin{equation}
            \var(\nu_{i,t}) = \rho^2 \nu_{i,t-1} + \var(\xi_{i,t}) 
            \quad\Leftrightarrow\quad
            \var(\nu_{i,t}) = \sigma_\nu^2 = \frac{\sigma_\xi^2}{1-\rho^2}, \quad\text{for any } t 
        \end{equation}

        sicne we observe the data $y_{i,t}$, we can compute the covariance between $y_{i,t}$ and $y_{i,t-1}$

        \[
            \cov(y_{i,t}, y_{i,t-1}) = \cov(\nu_{i,t}+\xi_{i,t}, \nu_{i,t-1}+\xi_{i,t-1})  
                                     \overset{\nu \perp \xi}{=} \sigma_\nu^2 
                                     \overset{(1)}{=} \frac{\sigma_\xi^2}{1-\rho^2}
        \]

        given that $\rho$ can be identified in question 1, we can conclude that $\sigma_\xi^2$ can also be identified.
        
        % Q1.3.2
        \subsubsection Next we show $\sigma_\epsilon^2$ is identified by considering the equation $y_{i,t} = \nu_{i,t} + \epsilon$, follow same procedure, we consider the variance of the terms
        
        \[ \begin{aligned}
            \var(y_{i,t}) &= \sigma_{\nu}^{2} + \sigma_{\epsilon}^{2}  \\
                          &\overset{(1)}{=} \frac{\sigma_\xi^2}{1-\rho^2} + \sigma_\epsilon^2
        \end{aligned} \]

        since $y_{i,t}$ is observed and we can compute $\var(y_{i,t})$, also we have shown that $\sigma_\xi^2$ and $\rho$ can be identified, we can conclude that $\sigma_\epsilon^2$ can be identified.

        % Q1.3.4
        \subsubsection hello

    
    % Q1.4 
    \subsection{Simulation of MLE}



% question 2
\section{Potential Outcome Framework}

    

        % Q2.1
        \subsection f
        \[
            w_i = D_iY_i(1) + (1-D_i)Y_i(0)
        \]
        
        % Q2.2
        \subsection f $Y_i(1)$ represents the outcome (wage) if individual $i$ choose to migrate; while $Y_i(0)$ represents the outcome if individual $i$ choose not to migrate
        \[ \left\{ \begin{aligned} 
            Y_i(1) = w_{i,1} = \mu_{i,1} + \epsilon_{i,1} \\
            Y_i(0) = w_{i,0} = \mu_{i,0} + \epsilon_{i,0}
        \end{aligned} \right. \]


        % Q2.3
        \subsubsection f $D_i \in \{0,1\}$ reflects on whether individual $i$ choose to migrate

    
    
    
% question 3
\section{Control for Observables}

    % Q3.1
    \subsection{Rosenbaum and Rubin}

        \[ \begin{aligned}
            P[D = 1|Y_0, Y_1, P] &= E[P[D = 1|Y_0, Y_1, P, X]|Y_0, Y_1, P] \\
            &= E[P[D = 1|Y_0, Y_1, X]|Y_0, Y_1, P], \quad (\cdot \text{ CIA}) \\
            &= E[P[D = 1|X]|Y_0, Y_1, P] \\
            &= E[P(X)|Y_0, Y_1, P] \\
            &= E[P|Y_0, Y_1, P] \\
            &= P
        \end{aligned} \]


    % Q3.2
    \subsection{Propensity Score}

        

            % Q3.2.1
            \subsubsection t 
            
            % Q3.2.2
            \subsubsection t
            
            % Q3.2.3
            \subsubsection t
            
            % Q3.2.4
            \subsubsection t
            
            % Q3.2.5
            \subsubsection t
            
            % Q3.2.6
            \subsubsection t
            
            % Q3.2.7
            \subsubsection t
            
            % Q3.2.8
            \subsubsection t
            
            % Q3.2.9
            \subsubsection t
            
            % Q3.2.10
            \subsubsection t
            
        







\end{document}  